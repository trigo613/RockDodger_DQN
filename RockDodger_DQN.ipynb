{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trigo613/RockDodger_DQN/blob/main/RockDodger_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from numpy.random import rand,randint\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "BD0GoscbgisD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dodger():\n",
        "    SCREEN_SIZE = (32,32)\n",
        "    PLAYER_SIZE = (2,2)\n",
        "\n",
        "    ROCK_MIN_HEIGHT = 2\n",
        "    ROCK_MAX_HEIGHT = 5\n",
        "\n",
        "    ROCK_MIN_WIDTH = 2\n",
        "    ROCK_MAX_WIDTH = 5\n",
        "\n",
        "    ROCK_SPEED = 2\n",
        "\n",
        "    PLAYER_Y = SCREEN_SIZE[0] - PLAYER_SIZE[0]*2 #Top (not middle)\n",
        "    PLAYER_INIT_X = int(SCREEN_SIZE[1]/2)\n",
        "\n",
        "    PLAYER_PIXEL = 2\n",
        "    ROCK_PIXEL = 1\n",
        "\n",
        "    ROCK_REWARD = 2\n",
        "\n",
        "    ROCK_PIXEL_NOISE = 0.5\n",
        "    EMPTY_PIXEL = 0\n",
        "\n",
        "    TIMER_MIN = 1\n",
        "    TIMER_MAX = 4\n",
        "\n",
        "    ACTION_SPACE = ['L','R','N']\n",
        "\n",
        "    def __init__(self):\n",
        "        self.grid = None\n",
        "        self.player_x = self.PLAYER_INIT_X\n",
        "        self.time_until_next_rock = self.TIMER_MIN\n",
        "        self.reward = 0\n",
        "        self.done = 0\n",
        "        self.rocks_list = [] #each element in list is of type {'x' : _, 'y' : _ , 'width' : _ , 'height' : _}\n",
        "\n",
        "    def __generate_grid_with_player(self):\n",
        "        self.grid = np.zeros(self.SCREEN_SIZE)\n",
        "        self.grid[self.PLAYER_Y:self.PLAYER_Y+self.PLAYER_SIZE[0],self.player_x:self.player_x+self.PLAYER_SIZE[1]] = self.PLAYER_PIXEL\n",
        "\n",
        "    def __restart_rock_counter(self):\n",
        "        self.time_until_next_rock = np.random.randint(self.TIMER_MIN,self.TIMER_MAX)\n",
        "\n",
        "    def __generate_rock(self):\n",
        "        self.time_until_next_rock -= 1\n",
        "        if self.time_until_next_rock > 0:\n",
        "            return\n",
        "        #Generating rock if time reached zero\n",
        "        rock_height =  np.random.randint(self.ROCK_MIN_HEIGHT,self.ROCK_MAX_HEIGHT)\n",
        "        rock_width = np.random.randint(self.ROCK_MIN_WIDTH,self.ROCK_MAX_WIDTH)\n",
        "        rock_y = - rock_height\n",
        "        if random.random() < 0.7:\n",
        "            rock_x = np.random.randint(0,self.SCREEN_SIZE[1] - rock_width + 1)\n",
        "        else:\n",
        "            rock_x = self.player_x\n",
        "            if rock_x + rock_width > self.SCREEN_SIZE[1]:\n",
        "                rock_x = self.SCREEN_SIZE[1] - rock_width\n",
        "        rock_color = self.ROCK_PIXEL + random.uniform(-self.ROCK_PIXEL_NOISE, self.ROCK_PIXEL_NOISE)\n",
        "        self.rocks_list.append({'x' : rock_x,\n",
        "                               'y' :rock_y ,\n",
        "                               'width' : rock_width ,\n",
        "                               'height' :rock_height,\n",
        "                               'color' : rock_color})\n",
        "        self.__restart_rock_counter()\n",
        "\n",
        "\n",
        "    def __render_rocks(self):\n",
        "        new_rocks_list = []\n",
        "        for rock in self.rocks_list:\n",
        "            rock['y'] += self.ROCK_SPEED\n",
        "            if rock['y'] < self.SCREEN_SIZE[0]:\n",
        "                y_start = max(0, rock['y'])\n",
        "                y_end = min(self.SCREEN_SIZE[0], rock['y'] + rock['height'])\n",
        "                self.grid[y_start:y_end, rock['x']:rock['x'] + rock['width']] = rock['color']\n",
        "                new_rocks_list.append(rock)\n",
        "            else:\n",
        "                self.reward += self.ROCK_REWARD\n",
        "        self.rocks_list = new_rocks_list\n",
        "\n",
        "\n",
        "    def __check_collisions(self):\n",
        "        collision_detected = False\n",
        "        for rock in self.rocks_list:\n",
        "            if (self.player_x < rock['x'] + rock['width'] and self.player_x + self.PLAYER_SIZE[1] > rock['x'] and\n",
        "                self.PLAYER_Y < rock['y'] + rock['height'] + 1 and self.PLAYER_Y + self.PLAYER_SIZE[0] > rock['y']):\n",
        "                self.reward -= 15\n",
        "                self.done = 1\n",
        "                break\n",
        "\n",
        "    def __move_player(self, action):\n",
        "        assert action in self.ACTION_SPACE\n",
        "        if action == 'L' and self.player_x > 0:\n",
        "                self.player_x -= 1\n",
        "        elif action == 'R' and self.player_x + self.PLAYER_SIZE[1] < self.SCREEN_SIZE[1]:\n",
        "                self.player_x += 1\n",
        "\n",
        "    def info(self):\n",
        "        return {\"scree_size\" : self.SCREEN_SIZE,\n",
        "                \"action_space\":self.ACTION_SPACE,\n",
        "                \"goal\": \"The goal of the game is for the\" +\n",
        "                        \"player to dodge all the rocks falling at him \" +\n",
        "                        \"by moving left (L) or right (R) or staying still (N).\" +\n",
        "                        \"every rock dodged earns him a point,\" +\n",
        "                        \"if the player collides with a rock then he loses\"}\n",
        "\n",
        "    def reset(self):\n",
        "        self.reward = 0\n",
        "        self.done = 0\n",
        "        self.player_x = self.PLAYER_INIT_X\n",
        "        self.__generate_grid_with_player()\n",
        "        self.__restart_rock_counter()\n",
        "        self.reward_earned = False\n",
        "        self.rocks_list = []\n",
        "        return {'state' : self.grid,\n",
        "                'reward':0,\n",
        "                'done' : 0}\n",
        "\n",
        "    def step(self,action):\n",
        "        self.reward = 0\n",
        "        self.done = 0\n",
        "        self.__move_player(action)\n",
        "        self.__generate_grid_with_player()\n",
        "        self.__generate_rock() #Checks if needs to generate a rock\n",
        "        self.__render_rocks() #Renders the rocks and updates the reward\n",
        "        self.__check_collisions()\n",
        "        return {'state' : self.grid,\n",
        "                'reward':self.reward,\n",
        "                'done' : self.done}"
      ],
      "metadata": {
        "id": "2Gm-lh1pgjNz"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "eVRJ29D3gpAR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Define the convolutional layers in a Sequential block\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Conv2d(128, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        linear_input_size = self.calculate_conv_output_size(h, w)\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    def calculate_conv_output_size(self, h, w):\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 1, h, w)\n",
        "            x = self.conv_layers(x)\n",
        "            x = self.flatten(x)\n",
        "        return x.size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "buWcOPO2CPLB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, n_actions, device):\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "        self.device = device\n",
        "\n",
        "        self.policy_net = CNN(state_shape[0], state_shape[1], n_actions).to(device)\n",
        "        self.target_net = CNN(state_shape[0], state_shape[1], n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.target_net.eval() #Turn off training for target model\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters())\n",
        "\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.target_update = 10\n",
        "\n",
        "        self.steps_done = 0\n",
        "        self.episodes_played = 0\n",
        "\n",
        "    def select_action(self, state, best = False):\n",
        "        if not best and random.random() > self.epsilon:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(self.device)\n",
        "                q_values = self.policy_net(state)\n",
        "                return torch.argmax(q_values, dim=1).item()\n",
        "        else:\n",
        "            return random.randrange(self.n_actions)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
        "\n",
        "        state_batch = np.array(state_batch)  # Convert the list of numpy arrays into a single numpy array\n",
        "        state_batch = torch.FloatTensor(state_batch).unsqueeze(1).to(self.device)\n",
        "\n",
        "        action_batch = torch.LongTensor(action_batch).to(self.device)\n",
        "\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device)\n",
        "\n",
        "        next_state_batch = np.array(next_state_batch)\n",
        "        next_state_batch = torch.FloatTensor(next_state_batch).unsqueeze(1).to(self.device)\n",
        "\n",
        "        done_batch = torch.FloatTensor(done_batch).to(self.device)\n",
        "\n",
        "        q_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
        "\n",
        "        next_q_values = torch.max(self.target_net(next_state_batch), dim=1)[0].detach()\n",
        "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
        "\n",
        "        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ],
      "metadata": {
        "id": "gdtcczrogreL"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_dqn_agent_best(env, agent, max_steps=2000):\n",
        "  state = env.reset()['state']\n",
        "  frames = [state]\n",
        "  for step in range(max_steps):\n",
        "      action = agent.select_action(state,best=True)\n",
        "      action_map = {0: 'L', 1: 'R', 2: 'N'}\n",
        "      next_state_dict = env.step(action_map[action])\n",
        "      next_state,done = next_state_dict['state'],next_state_dict['done']\n",
        "      frames.append(next_state)\n",
        "      state = next_state\n",
        "      if done:\n",
        "          break\n",
        "  return frames\n"
      ],
      "metadata": {
        "id": "RZ_J7xEt1brV"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "def test_and_record_dqn_agent(env, agent,output_path, n_episodes=1, max_steps=2000,):\n",
        "    frames = play_dqn_agent_best(env, agent, max_steps)\n",
        "    def upscale_image(image, scale_factor):\n",
        "        return cv2.resize(image, None, fx=scale_factor, fy=scale_factor,\n",
        "                      interpolation=cv2.INTER_NEAREST)\n",
        "    frames = [upscale_image(f,6) for f in frames]\n",
        "    with imageio.get_writer(output_path, mode='I', fps=20) as writer:\n",
        "      for frame in frames:\n",
        "        writer.append_data((frame * 255).astype('uint8'))\n"
      ],
      "metadata": {
        "id": "ZmkhJeUaz6cX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "def test_and_display_dqn_agent(env, agent, n_episodes=1, max_steps=2000):\n",
        "    def display_frames(frames, delay=0.1):\n",
        "      for frame in frames:\n",
        "        plt.imshow(frame, vmin=0, vmax=2)\n",
        "        plt.axis('off')  # Turn off axis labels\n",
        "        display(plt.gcf())  # Display the current frame\n",
        "        time.sleep(delay)  # Delay between frames\n",
        "        clear_output(wait=True)  # Clear the previous frameb\n",
        "    frames = play_dqn_agent_best(env, agent, max_steps)\n",
        "    display_frames(frames)"
      ],
      "metadata": {
        "id": "PCoF58SohXGC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_dqn_agent(env, agent, n_episodes=10_000, max_steps=10_000, record = False ,record_every = 200):\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()['state']\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.select_action(state)\n",
        "            action_map = {0: 'L', 1: 'R', 2: 'N'}\n",
        "            next_state_dict = env.step(action_map[action])\n",
        "            next_state, reward, done = next_state_dict['state'], next_state_dict['reward'], next_state_dict['done']\n",
        "\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            agent.replay()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if episode % agent.target_update == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        agent.episodes_played += 1\n",
        "        if record == True and agent.episodes_played % record_every == 0:\n",
        "          output_path = \"output\" + str(agent.episodes_played) + \".gif\"\n",
        "          print(\"recording agent and saving at \", output_path)\n",
        "          test_and_record_dqn_agent(env, agent,output_path)\n",
        "        print(f\"Episode: {agent.episodes_played}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")"
      ],
      "metadata": {
        "id": "EyQTOUECGwW6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Dodger()\n",
        "state_shape = env.SCREEN_SIZE\n",
        "n_actions = 3  # Left, Right, No-op\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "FpdmkkDfNCj1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(state_shape, n_actions, device)"
      ],
      "metadata": {
        "id": "IsM4CjhAguEp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A_2FpOQgwlr",
        "outputId": "6a80e0df-e759-49ad-d366-046326c09c38"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dqn_agent(env, agent,record = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kN1r-2kBgxRR",
        "outputId": "fe652df8-edac-4c5a-d093-63255110e4bf"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: -9, Epsilon: 1.00\n",
            "Episode: 2, Total Reward: -11, Epsilon: 1.00\n",
            "Episode: 3, Total Reward: -15, Epsilon: 1.00\n",
            "Episode: 4, Total Reward: -15, Epsilon: 0.94\n",
            "Episode: 5, Total Reward: -15, Epsilon: 0.86\n",
            "Episode: 6, Total Reward: -15, Epsilon: 0.79\n",
            "Episode: 7, Total Reward: -15, Epsilon: 0.72\n",
            "Episode: 8, Total Reward: -9, Epsilon: 0.64\n",
            "Episode: 9, Total Reward: -1, Epsilon: 0.55\n",
            "Episode: 10, Total Reward: -15, Epsilon: 0.51\n",
            "Episode: 11, Total Reward: -15, Epsilon: 0.48\n",
            "Episode: 12, Total Reward: -15, Epsilon: 0.44\n",
            "Episode: 13, Total Reward: 3, Epsilon: 0.38\n",
            "Episode: 14, Total Reward: -15, Epsilon: 0.35\n",
            "Episode: 15, Total Reward: -13, Epsilon: 0.32\n",
            "Episode: 16, Total Reward: 3, Epsilon: 0.27\n",
            "Episode: 17, Total Reward: -15, Epsilon: 0.25\n",
            "Episode: 18, Total Reward: -7, Epsilon: 0.22\n",
            "Episode: 19, Total Reward: -15, Epsilon: 0.20\n",
            "Episode: 20, Total Reward: -15, Epsilon: 0.19\n",
            "Episode: 21, Total Reward: -5, Epsilon: 0.16\n",
            "Episode: 22, Total Reward: 1, Epsilon: 0.14\n",
            "Episode: 23, Total Reward: 3, Epsilon: 0.12\n",
            "Episode: 24, Total Reward: -1, Epsilon: 0.10\n",
            "Episode: 25, Total Reward: -15, Epsilon: 0.09\n",
            "Episode: 26, Total Reward: -9, Epsilon: 0.08\n",
            "Episode: 27, Total Reward: -15, Epsilon: 0.07\n",
            "Episode: 28, Total Reward: -5, Epsilon: 0.07\n",
            "Episode: 29, Total Reward: -13, Epsilon: 0.06\n",
            "Episode: 30, Total Reward: -9, Epsilon: 0.05\n",
            "Episode: 31, Total Reward: 11, Epsilon: 0.04\n",
            "Episode: 32, Total Reward: -11, Epsilon: 0.04\n",
            "Episode: 33, Total Reward: -13, Epsilon: 0.04\n",
            "Episode: 34, Total Reward: 3, Epsilon: 0.03\n",
            "Episode: 35, Total Reward: 3, Epsilon: 0.02\n",
            "Episode: 36, Total Reward: 27, Epsilon: 0.02\n",
            "Episode: 37, Total Reward: -15, Epsilon: 0.02\n",
            "Episode: 38, Total Reward: -11, Epsilon: 0.02\n",
            "Episode: 39, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 40, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 41, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 42, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 43, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 44, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 45, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 46, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 47, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 48, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 49, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 50, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 51, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 52, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 53, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 54, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 55, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 56, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 57, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 58, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 59, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 60, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 61, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 62, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 63, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 64, Total Reward: 49, Epsilon: 0.01\n",
            "Episode: 65, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 66, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 67, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 68, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 69, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 70, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 71, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 72, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 73, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 74, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 75, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 76, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 77, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 78, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 79, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 80, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 81, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 82, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 83, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 84, Total Reward: 31, Epsilon: 0.01\n",
            "Episode: 85, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 86, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 87, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 88, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 89, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 90, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 91, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 92, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 93, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 94, Total Reward: 31, Epsilon: 0.01\n",
            "Episode: 95, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 96, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 97, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 98, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 99, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 100, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 101, Total Reward: 33, Epsilon: 0.01\n",
            "Episode: 102, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 103, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 104, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 105, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 106, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 107, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 108, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 109, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 110, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 111, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 112, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 113, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 114, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 115, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 116, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 117, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 118, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 119, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 120, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 121, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 122, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 123, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 124, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 125, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 126, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 127, Total Reward: 37, Epsilon: 0.01\n",
            "Episode: 128, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 129, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 130, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 131, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 132, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 133, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 134, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 135, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 136, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 137, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 138, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 139, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 140, Total Reward: 39, Epsilon: 0.01\n",
            "Episode: 141, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 142, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 143, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 144, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 145, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 146, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 147, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 148, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 149, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 150, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 151, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 152, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 153, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 154, Total Reward: 23, Epsilon: 0.01\n",
            "Episode: 155, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 156, Total Reward: 49, Epsilon: 0.01\n",
            "Episode: 157, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 158, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 159, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 160, Total Reward: 23, Epsilon: 0.01\n",
            "Episode: 161, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 162, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 163, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 164, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 165, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 166, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 167, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 168, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 169, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 170, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 171, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 172, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 173, Total Reward: 9, Epsilon: 0.01\n",
            "Episode: 174, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 175, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 176, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 177, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 178, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 179, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 180, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 181, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 182, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 183, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 184, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 185, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 186, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 187, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 188, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 189, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 190, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 191, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 192, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 193, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 194, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 195, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 196, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 197, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 198, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 199, Total Reward: -15, Epsilon: 0.01\n",
            "recording agent and saving at  output200.gif\n",
            "Episode: 200, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 201, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 202, Total Reward: 9, Epsilon: 0.01\n",
            "Episode: 203, Total Reward: 29, Epsilon: 0.01\n",
            "Episode: 204, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 205, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 206, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 207, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 208, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 209, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 210, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 211, Total Reward: 23, Epsilon: 0.01\n",
            "Episode: 212, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 213, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 214, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 215, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 216, Total Reward: 31, Epsilon: 0.01\n",
            "Episode: 217, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 218, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 219, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 220, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 221, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 222, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 223, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 224, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 225, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 226, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 227, Total Reward: 9, Epsilon: 0.01\n",
            "Episode: 228, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 229, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 230, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 231, Total Reward: 9, Epsilon: 0.01\n",
            "Episode: 232, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 233, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 234, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 235, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 236, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 237, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 238, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 239, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 240, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 241, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 242, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 243, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 244, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 245, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 246, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 247, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 248, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 249, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 250, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 251, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 252, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 253, Total Reward: 69, Epsilon: 0.01\n",
            "Episode: 254, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 255, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 256, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 257, Total Reward: 57, Epsilon: 0.01\n",
            "Episode: 258, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 259, Total Reward: 31, Epsilon: 0.01\n",
            "Episode: 260, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 261, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 262, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 263, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 264, Total Reward: 33, Epsilon: 0.01\n",
            "Episode: 265, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 266, Total Reward: 87, Epsilon: 0.01\n",
            "Episode: 267, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 268, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 269, Total Reward: 53, Epsilon: 0.01\n",
            "Episode: 270, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 271, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 272, Total Reward: 55, Epsilon: 0.01\n",
            "Episode: 273, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 274, Total Reward: 79, Epsilon: 0.01\n",
            "Episode: 275, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 276, Total Reward: 39, Epsilon: 0.01\n",
            "Episode: 277, Total Reward: 29, Epsilon: 0.01\n",
            "Episode: 278, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 279, Total Reward: 43, Epsilon: 0.01\n",
            "Episode: 280, Total Reward: 47, Epsilon: 0.01\n",
            "Episode: 281, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 282, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 283, Total Reward: 9, Epsilon: 0.01\n",
            "Episode: 284, Total Reward: 65, Epsilon: 0.01\n",
            "Episode: 285, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 286, Total Reward: 23, Epsilon: 0.01\n",
            "Episode: 287, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 288, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 289, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 290, Total Reward: 81, Epsilon: 0.01\n",
            "Episode: 291, Total Reward: 77, Epsilon: 0.01\n",
            "Episode: 292, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 293, Total Reward: 41, Epsilon: 0.01\n",
            "Episode: 294, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 295, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 296, Total Reward: 57, Epsilon: 0.01\n",
            "Episode: 297, Total Reward: 63, Epsilon: 0.01\n",
            "Episode: 298, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 299, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 300, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 301, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 302, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 303, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 304, Total Reward: 29, Epsilon: 0.01\n",
            "Episode: 305, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 306, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 307, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 308, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 309, Total Reward: 83, Epsilon: 0.01\n",
            "Episode: 310, Total Reward: 97, Epsilon: 0.01\n",
            "Episode: 311, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 312, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 313, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 314, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 315, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 316, Total Reward: 179, Epsilon: 0.01\n",
            "Episode: 317, Total Reward: 45, Epsilon: 0.01\n",
            "Episode: 318, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 319, Total Reward: 73, Epsilon: 0.01\n",
            "Episode: 320, Total Reward: 73, Epsilon: 0.01\n",
            "Episode: 321, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 322, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 323, Total Reward: 239, Epsilon: 0.01\n",
            "Episode: 324, Total Reward: 9, Epsilon: 0.01\n",
            "Episode: 325, Total Reward: 43, Epsilon: 0.01\n",
            "Episode: 326, Total Reward: 83, Epsilon: 0.01\n",
            "Episode: 327, Total Reward: 73, Epsilon: 0.01\n",
            "Episode: 328, Total Reward: 61, Epsilon: 0.01\n",
            "Episode: 329, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 330, Total Reward: 151, Epsilon: 0.01\n",
            "Episode: 331, Total Reward: 135, Epsilon: 0.01\n",
            "Episode: 332, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 333, Total Reward: 161, Epsilon: 0.01\n",
            "Episode: 334, Total Reward: 53, Epsilon: 0.01\n",
            "Episode: 335, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 336, Total Reward: 141, Epsilon: 0.01\n",
            "Episode: 337, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 338, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 339, Total Reward: 119, Epsilon: 0.01\n",
            "Episode: 340, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 341, Total Reward: 69, Epsilon: 0.01\n",
            "Episode: 342, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 343, Total Reward: 81, Epsilon: 0.01\n",
            "Episode: 344, Total Reward: 75, Epsilon: 0.01\n",
            "Episode: 345, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 346, Total Reward: 79, Epsilon: 0.01\n",
            "Episode: 347, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 348, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 349, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 350, Total Reward: 145, Epsilon: 0.01\n",
            "Episode: 351, Total Reward: 209, Epsilon: 0.01\n",
            "Episode: 352, Total Reward: 61, Epsilon: 0.01\n",
            "Episode: 353, Total Reward: 49, Epsilon: 0.01\n",
            "Episode: 354, Total Reward: 65, Epsilon: 0.01\n",
            "Episode: 355, Total Reward: 81, Epsilon: 0.01\n",
            "Episode: 356, Total Reward: 79, Epsilon: 0.01\n",
            "Episode: 357, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 358, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 359, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 360, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 361, Total Reward: 49, Epsilon: 0.01\n",
            "Episode: 362, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 363, Total Reward: 51, Epsilon: 0.01\n",
            "Episode: 364, Total Reward: 57, Epsilon: 0.01\n",
            "Episode: 365, Total Reward: 261, Epsilon: 0.01\n",
            "Episode: 366, Total Reward: 129, Epsilon: 0.01\n",
            "Episode: 367, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 368, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 369, Total Reward: 153, Epsilon: 0.01\n",
            "Episode: 370, Total Reward: 203, Epsilon: 0.01\n",
            "Episode: 371, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 372, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 373, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 374, Total Reward: 107, Epsilon: 0.01\n",
            "Episode: 375, Total Reward: 131, Epsilon: 0.01\n",
            "Episode: 376, Total Reward: 289, Epsilon: 0.01\n",
            "Episode: 377, Total Reward: 95, Epsilon: 0.01\n",
            "Episode: 378, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 379, Total Reward: 79, Epsilon: 0.01\n",
            "Episode: 380, Total Reward: 43, Epsilon: 0.01\n",
            "Episode: 381, Total Reward: 47, Epsilon: 0.01\n",
            "Episode: 382, Total Reward: 41, Epsilon: 0.01\n",
            "Episode: 383, Total Reward: 31, Epsilon: 0.01\n",
            "Episode: 384, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 385, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 386, Total Reward: 31, Epsilon: 0.01\n",
            "Episode: 387, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 388, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 389, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 390, Total Reward: 31, Epsilon: 0.01\n",
            "Episode: 391, Total Reward: 147, Epsilon: 0.01\n",
            "Episode: 392, Total Reward: 89, Epsilon: 0.01\n",
            "Episode: 393, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 394, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 395, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 396, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 397, Total Reward: 43, Epsilon: 0.01\n",
            "Episode: 398, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 399, Total Reward: 75, Epsilon: 0.01\n",
            "recording agent and saving at  output400.gif\n",
            "Episode: 400, Total Reward: 83, Epsilon: 0.01\n",
            "Episode: 401, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 402, Total Reward: 207, Epsilon: 0.01\n",
            "Episode: 403, Total Reward: 129, Epsilon: 0.01\n",
            "Episode: 404, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 405, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 406, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 407, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 408, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 409, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 410, Total Reward: 217, Epsilon: 0.01\n",
            "Episode: 411, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 412, Total Reward: 73, Epsilon: 0.01\n",
            "Episode: 413, Total Reward: 77, Epsilon: 0.01\n",
            "Episode: 414, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 415, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 416, Total Reward: 105, Epsilon: 0.01\n",
            "Episode: 417, Total Reward: 97, Epsilon: 0.01\n",
            "Episode: 418, Total Reward: 207, Epsilon: 0.01\n",
            "Episode: 419, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 420, Total Reward: 37, Epsilon: 0.01\n",
            "Episode: 421, Total Reward: 173, Epsilon: 0.01\n",
            "Episode: 422, Total Reward: 107, Epsilon: 0.01\n",
            "Episode: 423, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 424, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 425, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 426, Total Reward: 85, Epsilon: 0.01\n",
            "Episode: 427, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 428, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 429, Total Reward: 33, Epsilon: 0.01\n",
            "Episode: 430, Total Reward: 317, Epsilon: 0.01\n",
            "Episode: 431, Total Reward: 55, Epsilon: 0.01\n",
            "Episode: 432, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 433, Total Reward: 189, Epsilon: 0.01\n",
            "Episode: 434, Total Reward: 271, Epsilon: 0.01\n",
            "Episode: 435, Total Reward: 53, Epsilon: 0.01\n",
            "Episode: 436, Total Reward: 345, Epsilon: 0.01\n",
            "Episode: 437, Total Reward: -7, Epsilon: 0.01\n",
            "Episode: 438, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 439, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 440, Total Reward: 51, Epsilon: 0.01\n",
            "Episode: 441, Total Reward: 53, Epsilon: 0.01\n",
            "Episode: 442, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 443, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 444, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 445, Total Reward: 137, Epsilon: 0.01\n",
            "Episode: 446, Total Reward: 87, Epsilon: 0.01\n",
            "Episode: 447, Total Reward: 47, Epsilon: 0.01\n",
            "Episode: 448, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 449, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 450, Total Reward: 29, Epsilon: 0.01\n",
            "Episode: 451, Total Reward: 137, Epsilon: 0.01\n",
            "Episode: 452, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 453, Total Reward: 113, Epsilon: 0.01\n",
            "Episode: 454, Total Reward: 259, Epsilon: 0.01\n",
            "Episode: 455, Total Reward: 105, Epsilon: 0.01\n",
            "Episode: 456, Total Reward: 89, Epsilon: 0.01\n",
            "Episode: 457, Total Reward: 29, Epsilon: 0.01\n",
            "Episode: 458, Total Reward: 49, Epsilon: 0.01\n",
            "Episode: 459, Total Reward: 191, Epsilon: 0.01\n",
            "Episode: 460, Total Reward: 45, Epsilon: 0.01\n",
            "Episode: 461, Total Reward: 87, Epsilon: 0.01\n",
            "Episode: 462, Total Reward: 51, Epsilon: 0.01\n",
            "Episode: 463, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 464, Total Reward: 119, Epsilon: 0.01\n",
            "Episode: 465, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 466, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 467, Total Reward: 51, Epsilon: 0.01\n",
            "Episode: 468, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 469, Total Reward: 29, Epsilon: 0.01\n",
            "Episode: 470, Total Reward: 223, Epsilon: 0.01\n",
            "Episode: 471, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 472, Total Reward: 61, Epsilon: 0.01\n",
            "Episode: 473, Total Reward: 235, Epsilon: 0.01\n",
            "Episode: 474, Total Reward: 37, Epsilon: 0.01\n",
            "Episode: 475, Total Reward: 95, Epsilon: 0.01\n",
            "Episode: 476, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 477, Total Reward: 81, Epsilon: 0.01\n",
            "Episode: 478, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 479, Total Reward: 159, Epsilon: 0.01\n",
            "Episode: 480, Total Reward: 45, Epsilon: 0.01\n",
            "Episode: 481, Total Reward: 201, Epsilon: 0.01\n",
            "Episode: 482, Total Reward: 177, Epsilon: 0.01\n",
            "Episode: 483, Total Reward: 61, Epsilon: 0.01\n",
            "Episode: 484, Total Reward: 45, Epsilon: 0.01\n",
            "Episode: 485, Total Reward: 263, Epsilon: 0.01\n",
            "Episode: 486, Total Reward: 95, Epsilon: 0.01\n",
            "Episode: 487, Total Reward: 135, Epsilon: 0.01\n",
            "Episode: 488, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 489, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 490, Total Reward: 85, Epsilon: 0.01\n",
            "Episode: 491, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 492, Total Reward: 9, Epsilon: 0.01\n",
            "Episode: 493, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 494, Total Reward: 37, Epsilon: 0.01\n",
            "Episode: 495, Total Reward: 215, Epsilon: 0.01\n",
            "Episode: 496, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 497, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 498, Total Reward: 399, Epsilon: 0.01\n",
            "Episode: 499, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 500, Total Reward: 19, Epsilon: 0.01\n",
            "Episode: 501, Total Reward: 37, Epsilon: 0.01\n",
            "Episode: 502, Total Reward: 23, Epsilon: 0.01\n",
            "Episode: 503, Total Reward: 235, Epsilon: 0.01\n",
            "Episode: 504, Total Reward: 53, Epsilon: 0.01\n",
            "Episode: 505, Total Reward: 233, Epsilon: 0.01\n",
            "Episode: 506, Total Reward: 147, Epsilon: 0.01\n",
            "Episode: 507, Total Reward: 51, Epsilon: 0.01\n",
            "Episode: 508, Total Reward: 99, Epsilon: 0.01\n",
            "Episode: 509, Total Reward: 21, Epsilon: 0.01\n",
            "Episode: 510, Total Reward: 213, Epsilon: 0.01\n",
            "Episode: 511, Total Reward: 307, Epsilon: 0.01\n",
            "Episode: 512, Total Reward: 81, Epsilon: 0.01\n",
            "Episode: 513, Total Reward: 107, Epsilon: 0.01\n",
            "Episode: 514, Total Reward: 217, Epsilon: 0.01\n",
            "Episode: 515, Total Reward: 23, Epsilon: 0.01\n",
            "Episode: 516, Total Reward: 77, Epsilon: 0.01\n",
            "Episode: 517, Total Reward: 81, Epsilon: 0.01\n",
            "Episode: 518, Total Reward: 107, Epsilon: 0.01\n",
            "Episode: 519, Total Reward: 167, Epsilon: 0.01\n",
            "Episode: 520, Total Reward: 45, Epsilon: 0.01\n",
            "Episode: 521, Total Reward: 153, Epsilon: 0.01\n",
            "Episode: 522, Total Reward: 49, Epsilon: 0.01\n",
            "Episode: 523, Total Reward: 17, Epsilon: 0.01\n",
            "Episode: 524, Total Reward: 119, Epsilon: 0.01\n",
            "Episode: 525, Total Reward: 91, Epsilon: 0.01\n",
            "Episode: 526, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 527, Total Reward: 153, Epsilon: 0.01\n",
            "Episode: 528, Total Reward: 43, Epsilon: 0.01\n",
            "Episode: 529, Total Reward: 11, Epsilon: 0.01\n",
            "Episode: 530, Total Reward: 83, Epsilon: 0.01\n",
            "Episode: 531, Total Reward: -13, Epsilon: 0.01\n",
            "Episode: 532, Total Reward: 87, Epsilon: 0.01\n",
            "Episode: 533, Total Reward: 33, Epsilon: 0.01\n",
            "Episode: 534, Total Reward: -11, Epsilon: 0.01\n",
            "Episode: 535, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 536, Total Reward: 137, Epsilon: 0.01\n",
            "Episode: 537, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 538, Total Reward: 77, Epsilon: 0.01\n",
            "Episode: 539, Total Reward: 227, Epsilon: 0.01\n",
            "Episode: 540, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 541, Total Reward: 57, Epsilon: 0.01\n",
            "Episode: 542, Total Reward: 47, Epsilon: 0.01\n",
            "Episode: 543, Total Reward: 29, Epsilon: 0.01\n",
            "Episode: 544, Total Reward: 265, Epsilon: 0.01\n",
            "Episode: 545, Total Reward: 117, Epsilon: 0.01\n",
            "Episode: 546, Total Reward: 13, Epsilon: 0.01\n",
            "Episode: 547, Total Reward: 57, Epsilon: 0.01\n",
            "Episode: 548, Total Reward: 101, Epsilon: 0.01\n",
            "Episode: 549, Total Reward: 55, Epsilon: 0.01\n",
            "Episode: 550, Total Reward: 187, Epsilon: 0.01\n",
            "Episode: 551, Total Reward: 5, Epsilon: 0.01\n",
            "Episode: 552, Total Reward: 177, Epsilon: 0.01\n",
            "Episode: 553, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 554, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 555, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 556, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 557, Total Reward: 343, Epsilon: 0.01\n",
            "Episode: 558, Total Reward: 87, Epsilon: 0.01\n",
            "Episode: 559, Total Reward: 83, Epsilon: 0.01\n",
            "Episode: 560, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 561, Total Reward: -9, Epsilon: 0.01\n",
            "Episode: 562, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 563, Total Reward: 31, Epsilon: 0.01\n",
            "Episode: 564, Total Reward: 79, Epsilon: 0.01\n",
            "Episode: 565, Total Reward: 89, Epsilon: 0.01\n",
            "Episode: 566, Total Reward: 97, Epsilon: 0.01\n",
            "Episode: 567, Total Reward: 47, Epsilon: 0.01\n",
            "Episode: 568, Total Reward: 87, Epsilon: 0.01\n",
            "Episode: 569, Total Reward: 149, Epsilon: 0.01\n",
            "Episode: 570, Total Reward: 23, Epsilon: 0.01\n",
            "Episode: 571, Total Reward: 473, Epsilon: 0.01\n",
            "Episode: 572, Total Reward: 7, Epsilon: 0.01\n",
            "Episode: 573, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 574, Total Reward: 73, Epsilon: 0.01\n",
            "Episode: 575, Total Reward: 73, Epsilon: 0.01\n",
            "Episode: 576, Total Reward: 131, Epsilon: 0.01\n",
            "Episode: 577, Total Reward: 1, Epsilon: 0.01\n",
            "Episode: 578, Total Reward: 191, Epsilon: 0.01\n",
            "Episode: 579, Total Reward: 39, Epsilon: 0.01\n",
            "Episode: 580, Total Reward: 135, Epsilon: 0.01\n",
            "Episode: 581, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 582, Total Reward: 145, Epsilon: 0.01\n",
            "Episode: 583, Total Reward: 147, Epsilon: 0.01\n",
            "Episode: 584, Total Reward: 33, Epsilon: 0.01\n",
            "Episode: 585, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 586, Total Reward: 3, Epsilon: 0.01\n",
            "Episode: 587, Total Reward: 143, Epsilon: 0.01\n",
            "Episode: 588, Total Reward: 35, Epsilon: 0.01\n",
            "Episode: 589, Total Reward: 33, Epsilon: 0.01\n",
            "Episode: 590, Total Reward: 29, Epsilon: 0.01\n",
            "Episode: 591, Total Reward: 59, Epsilon: 0.01\n",
            "Episode: 592, Total Reward: 145, Epsilon: 0.01\n",
            "Episode: 593, Total Reward: 489, Epsilon: 0.01\n",
            "Episode: 594, Total Reward: 205, Epsilon: 0.01\n",
            "Episode: 595, Total Reward: 139, Epsilon: 0.01\n",
            "Episode: 596, Total Reward: 45, Epsilon: 0.01\n",
            "Episode: 597, Total Reward: 53, Epsilon: 0.01\n",
            "Episode: 598, Total Reward: 229, Epsilon: 0.01\n",
            "Episode: 599, Total Reward: 195, Epsilon: 0.01\n",
            "recording agent and saving at  output600.gif\n",
            "Episode: 600, Total Reward: 75, Epsilon: 0.01\n",
            "Episode: 601, Total Reward: 201, Epsilon: 0.01\n",
            "Episode: 602, Total Reward: 37, Epsilon: 0.01\n",
            "Episode: 603, Total Reward: 51, Epsilon: 0.01\n",
            "Episode: 604, Total Reward: 77, Epsilon: 0.01\n",
            "Episode: 605, Total Reward: 141, Epsilon: 0.01\n",
            "Episode: 606, Total Reward: 225, Epsilon: 0.01\n",
            "Episode: 607, Total Reward: 431, Epsilon: 0.01\n",
            "Episode: 608, Total Reward: 143, Epsilon: 0.01\n",
            "Episode: 609, Total Reward: -1, Epsilon: 0.01\n",
            "Episode: 610, Total Reward: 117, Epsilon: 0.01\n",
            "Episode: 611, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 612, Total Reward: -15, Epsilon: 0.01\n",
            "Episode: 613, Total Reward: 395, Epsilon: 0.01\n",
            "Episode: 614, Total Reward: 27, Epsilon: 0.01\n",
            "Episode: 615, Total Reward: -3, Epsilon: 0.01\n",
            "Episode: 616, Total Reward: 71, Epsilon: 0.01\n",
            "Episode: 617, Total Reward: 105, Epsilon: 0.01\n",
            "Episode: 618, Total Reward: 159, Epsilon: 0.01\n",
            "Episode: 619, Total Reward: 443, Epsilon: 0.01\n",
            "Episode: 620, Total Reward: 15, Epsilon: 0.01\n",
            "Episode: 621, Total Reward: 99, Epsilon: 0.01\n",
            "Episode: 622, Total Reward: 83, Epsilon: 0.01\n",
            "Episode: 623, Total Reward: 97, Epsilon: 0.01\n",
            "Episode: 624, Total Reward: 25, Epsilon: 0.01\n",
            "Episode: 625, Total Reward: -5, Epsilon: 0.01\n",
            "Episode: 626, Total Reward: 133, Epsilon: 0.01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-e41ba057ebb6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dqn_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-26177843ca3f>\u001b[0m in \u001b[0;36mtrain_dqn_agent\u001b[0;34m(env, agent, n_episodes, max_steps, record, record_every)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-885335d2d5ca>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mnext_state_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mnext_state_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mdone_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = Dodger()\n",
        "test_and_record_dqn_agent(env, agent,output_path='resized_output_RSMprop.gif')"
      ],
      "metadata": {
        "id": "D--zrnAi0FMx"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Dodger()\n",
        "test_and_display_dqn_agent(env, agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QVqROMLJhSKR",
        "outputId": "baac47f6-72f0-41c9-9a42-779c8285c6b3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a54ade66a144>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDodger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_and_display_dqn_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-d29e39890891>\u001b[0m in \u001b[0;36mtest_and_display_dqn_agent\u001b[0;34m(env, agent, n_episodes, max_steps)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear the previous frameb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_dqn_agent_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdisplay_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-d29e39890891>\u001b[0m in \u001b[0;36mdisplay_frames\u001b[0;34m(frames, delay)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Turn off axis labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Display the current frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Delay between frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear the previous frameb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_dqn_agent_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yu_wHzWehVB5"
      },
      "execution_count": 3,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOcYENsScQgl4HwzmzTGJjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}