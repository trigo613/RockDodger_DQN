{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trigo613/RockDodger_DQN/blob/main/RockDodger_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from numpy.random import rand,randint\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "BD0GoscbgisD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dodger():\n",
        "    SCREEN_SIZE = (32,32)\n",
        "    PLAYER_SIZE = (2,2)\n",
        "\n",
        "    ROCK_MIN_HEIGHT = 2\n",
        "    ROCK_MAX_HEIGHT = 5\n",
        "\n",
        "    ROCK_MIN_WIDTH = 2\n",
        "    ROCK_MAX_WIDTH = 5\n",
        "\n",
        "    ROCK_SPEED = 2\n",
        "\n",
        "    PLAYER_Y = SCREEN_SIZE[0] - PLAYER_SIZE[0]*2 #Top (not middle)\n",
        "    PLAYER_INIT_X = int(SCREEN_SIZE[1]/2)\n",
        "\n",
        "    PLAYER_PIXEL = 2\n",
        "    ROCK_PIXEL = 1\n",
        "\n",
        "    ROCK_REWARD = 2\n",
        "\n",
        "    ROCK_PIXEL_NOISE = 0.5\n",
        "    EMPTY_PIXEL = 0\n",
        "\n",
        "    TIMER_MIN = 1\n",
        "    TIMER_MAX = 4\n",
        "\n",
        "    ACTION_SPACE = ['L','R','N']\n",
        "\n",
        "    def __init__(self):\n",
        "        self.grid = None\n",
        "        self.player_x = self.PLAYER_INIT_X\n",
        "        self.time_until_next_rock = self.TIMER_MIN\n",
        "        self.reward = 0\n",
        "        self.done = 0\n",
        "        self.rocks_list = [] #each element in list is of type {'x' : _, 'y' : _ , 'width' : _ , 'height' : _}\n",
        "\n",
        "    def __generate_grid_with_player(self):\n",
        "        self.grid = np.zeros(self.SCREEN_SIZE)\n",
        "        self.grid[self.PLAYER_Y:self.PLAYER_Y+self.PLAYER_SIZE[0],self.player_x:self.player_x+self.PLAYER_SIZE[1]] = self.PLAYER_PIXEL\n",
        "\n",
        "    def __restart_rock_counter(self):\n",
        "        self.time_until_next_rock = np.random.randint(self.TIMER_MIN,self.TIMER_MAX)\n",
        "\n",
        "    def __generate_rock(self):\n",
        "        self.time_until_next_rock -= 1\n",
        "        if self.time_until_next_rock > 0:\n",
        "            return\n",
        "        #Generating rock if time reached zero\n",
        "        rock_height =  np.random.randint(self.ROCK_MIN_HEIGHT,self.ROCK_MAX_HEIGHT)\n",
        "        rock_width = np.random.randint(self.ROCK_MIN_WIDTH,self.ROCK_MAX_WIDTH)\n",
        "        rock_y = - rock_height\n",
        "        if random.random() < 0.7:\n",
        "            rock_x = np.random.randint(0,self.SCREEN_SIZE[1] - rock_width + 1)\n",
        "        else:\n",
        "            rock_x = self.player_x\n",
        "            if rock_x + rock_width > self.SCREEN_SIZE[1]:\n",
        "                rock_x = self.SCREEN_SIZE[1] - rock_width\n",
        "        rock_color = self.ROCK_PIXEL + random.uniform(-self.ROCK_PIXEL_NOISE, self.ROCK_PIXEL_NOISE)\n",
        "        self.rocks_list.append({'x' : rock_x,\n",
        "                               'y' :rock_y ,\n",
        "                               'width' : rock_width ,\n",
        "                               'height' :rock_height,\n",
        "                               'color' : rock_color})\n",
        "        self.__restart_rock_counter()\n",
        "\n",
        "\n",
        "    def __render_rocks(self):\n",
        "        new_rocks_list = []\n",
        "        for rock in self.rocks_list:\n",
        "            rock['y'] += self.ROCK_SPEED\n",
        "            if rock['y'] < self.SCREEN_SIZE[0]:\n",
        "                y_start = max(0, rock['y'])\n",
        "                y_end = min(self.SCREEN_SIZE[0], rock['y'] + rock['height'])\n",
        "                self.grid[y_start:y_end, rock['x']:rock['x'] + rock['width']] = rock['color']\n",
        "                new_rocks_list.append(rock)\n",
        "            else:\n",
        "                self.reward += self.ROCK_REWARD\n",
        "        self.rocks_list = new_rocks_list\n",
        "\n",
        "\n",
        "    def __check_collisions(self):\n",
        "        collision_detected = False\n",
        "        for rock in self.rocks_list:\n",
        "            if (self.player_x < rock['x'] + rock['width'] and self.player_x + self.PLAYER_SIZE[1] > rock['x'] and\n",
        "                self.PLAYER_Y < rock['y'] + rock['height'] + 1 and self.PLAYER_Y + self.PLAYER_SIZE[0] > rock['y']):\n",
        "                self.reward -= 15\n",
        "                self.done = 1\n",
        "                break\n",
        "\n",
        "    def __move_player(self, action):\n",
        "        assert action in self.ACTION_SPACE\n",
        "        if action == 'L' and self.player_x > 0:\n",
        "                self.player_x -= 1\n",
        "        elif action == 'R' and self.player_x + self.PLAYER_SIZE[1] < self.SCREEN_SIZE[1]:\n",
        "                self.player_x += 1\n",
        "\n",
        "    def info(self):\n",
        "        return {\"scree_size\" : self.SCREEN_SIZE,\n",
        "                \"action_space\":self.ACTION_SPACE,\n",
        "                \"goal\": \"The goal of the game is for the\" +\n",
        "                        \"player to dodge all the rocks falling at him \" +\n",
        "                        \"by moving left (L) or right (R) or staying still (N).\" +\n",
        "                        \"every rock dodged earns him a point,\" +\n",
        "                        \"if the player collides with a rock then he loses\"}\n",
        "\n",
        "    def reset(self):\n",
        "        self.reward = 0\n",
        "        self.done = 0\n",
        "        self.player_x = self.PLAYER_INIT_X\n",
        "        self.__generate_grid_with_player()\n",
        "        self.__restart_rock_counter()\n",
        "        self.reward_earned = False\n",
        "        self.rocks_list = []\n",
        "        return {'state' : self.grid,\n",
        "                'reward':0,\n",
        "                'done' : 0}\n",
        "\n",
        "    def step(self,action):\n",
        "        self.reward = 0\n",
        "        self.done = 0\n",
        "        self.__move_player(action)\n",
        "        self.__generate_grid_with_player()\n",
        "        self.__generate_rock() #Checks if needs to generate a rock\n",
        "        self.__render_rocks() #Renders the rocks and updates the reward\n",
        "        self.__check_collisions()\n",
        "        return {'state' : self.grid,\n",
        "                'reward':self.reward,\n",
        "                'done' : self.done}"
      ],
      "metadata": {
        "id": "2Gm-lh1pgjNz"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "eVRJ29D3gpAR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Define the convolutional layers in a Sequential block\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Conv2d(128, 128, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        linear_input_size = self.calculate_conv_output_size(h, w)\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    def calculate_conv_output_size(self, h, w):\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 1, h, w)\n",
        "            x = self.conv_layers(x)\n",
        "            x = self.flatten(x)\n",
        "        return x.size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "buWcOPO2CPLB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, n_actions, device):\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "        self.device = device\n",
        "\n",
        "        self.policy_net = CNN(state_shape[0], state_shape[1], n_actions).to(device)\n",
        "        self.target_net = CNN(state_shape[0], state_shape[1], n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.target_net.eval() #Turn off training for target model\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters())\n",
        "\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.target_update = 10\n",
        "\n",
        "        self.steps_done = 0\n",
        "        self.episodes_played = 0\n",
        "\n",
        "    def select_action(self, state, best = False):\n",
        "        if random.random() > self.epsilon or best:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(self.device)\n",
        "                q_values = self.policy_net(state)\n",
        "                return torch.argmax(q_values, dim=1).item()\n",
        "        else:\n",
        "            return random.randrange(self.n_actions)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
        "\n",
        "        state_batch = np.array(state_batch)  # Convert the list of numpy arrays into a single numpy array\n",
        "        state_batch = torch.FloatTensor(state_batch).unsqueeze(1).to(self.device)\n",
        "\n",
        "        action_batch = torch.LongTensor(action_batch).to(self.device)\n",
        "\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device)\n",
        "\n",
        "        next_state_batch = np.array(next_state_batch)\n",
        "        next_state_batch = torch.FloatTensor(next_state_batch).unsqueeze(1).to(self.device)\n",
        "\n",
        "        done_batch = torch.FloatTensor(done_batch).to(self.device)\n",
        "\n",
        "        q_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
        "\n",
        "        next_q_values = torch.max(self.target_net(next_state_batch), dim=1)[0].detach()\n",
        "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
        "\n",
        "        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ],
      "metadata": {
        "id": "gdtcczrogreL"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_dqn_agent_best(env, agent, max_steps=2000):\n",
        "  state = env.reset()['state']\n",
        "  frames = [state]\n",
        "  for step in range(max_steps):\n",
        "      action = agent.select_action(state,best=True)\n",
        "      action_map = {0: 'L', 1: 'R', 2: 'N'}\n",
        "      next_state_dict = env.step(action_map[action])\n",
        "      next_state,done = next_state_dict['state'],next_state_dict['done']\n",
        "      frames.append(next_state)\n",
        "      state = next_state\n",
        "      if done:\n",
        "          break\n",
        "  return frames\n"
      ],
      "metadata": {
        "id": "RZ_J7xEt1brV"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tests the agent and record the result in a gif file\n",
        "import imageio\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "def test_and_record_dqn_agent(env, agent,output_path, n_episodes=1, max_steps=2000,scale = 6,fps=20):\n",
        "    frames = play_dqn_agent_best(env, agent, max_steps)\n",
        "    def upscale_image(image, scale_factor):\n",
        "        return cv2.resize(image, None, fx=scale_factor, fy=scale_factor,\n",
        "                      interpolation=cv2.INTER_NEAREST)\n",
        "    frames = [upscale_image(f,scale) for f in frames]\n",
        "    with imageio.get_writer(output_path, mode='I', fps=fps) as writer:\n",
        "      for frame in frames:\n",
        "        writer.append_data((frame * 255).astype('uint8'))\n"
      ],
      "metadata": {
        "id": "ZmkhJeUaz6cX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For displaying the result in the notebook\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "def test_and_display_dqn_agent(env, agent, n_episodes=1, max_steps=2000):\n",
        "    def display_frames(frames, delay=0.1):\n",
        "      for frame in frames:\n",
        "        plt.imshow(frame, vmin=0, vmax=2)\n",
        "        plt.axis('off')  # Turn off axis labels\n",
        "        display(plt.gcf())  # Display the current frame\n",
        "        time.sleep(delay)  # Delay between frames\n",
        "        clear_output(wait=True)  # Clear the previous frameb\n",
        "    frames = play_dqn_agent_best(env, agent, max_steps)\n",
        "    display_frames(frames)"
      ],
      "metadata": {
        "id": "PCoF58SohXGC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_dqn_agent(env, agent, n_episodes=10_000, max_steps=10_000, record = False ,record_every = 200):\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()['state']\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.select_action(state)\n",
        "            action_map = {0: 'L', 1: 'R', 2: 'N'}\n",
        "            next_state_dict = env.step(action_map[action])\n",
        "            next_state, reward, done = next_state_dict['state'], next_state_dict['reward'], next_state_dict['done']\n",
        "\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            agent.replay()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if episode % agent.target_update == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        agent.episodes_played += 1\n",
        "        if record == True and agent.episodes_played % record_every == 0:\n",
        "          output_path = \"output\" + str(agent.episodes_played) + \".gif\"\n",
        "          print(\"recording agent and saving at \", output_path)\n",
        "          test_and_record_dqn_agent(env, agent,output_path)\n",
        "        print(f\"Episode: {agent.episodes_played}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")"
      ],
      "metadata": {
        "id": "EyQTOUECGwW6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Dodger()\n",
        "state_shape = env.SCREEN_SIZE\n",
        "n_actions = 3  # Left, Right, No-op\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "FpdmkkDfNCj1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(state_shape, n_actions, device)"
      ],
      "metadata": {
        "id": "IsM4CjhAguEp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verifying we are using the GPU\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A_2FpOQgwlr",
        "outputId": "6a80e0df-e759-49ad-d366-046326c09c38"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dqn_agent(env, agent,record = False)"
      ],
      "metadata": {
        "id": "4rxbT3mqRzYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tests the agent and record the result in a gif file\n",
        "env = Dodger()\n",
        "test_and_record_dqn_agent(env, agent,output_path='output.gif')"
      ],
      "metadata": {
        "id": "D--zrnAi0FMx"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tests the agent and displayes the result in the notebook\n",
        "env = Dodger()\n",
        "test_and_display_dqn_agent(env, agent)"
      ],
      "metadata": {
        "id": "wocqVfDJSnma"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNy+0W5JT+EE/a3S/ipfhhX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}